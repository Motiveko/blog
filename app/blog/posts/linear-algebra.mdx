---
title: "개발자를 위한 선형대수학"
description: "개발자를 위한 선형대수학 스터디"
keywords:["선형대수학"]
lang: "ko"
date: "2025-05-04"
publishedAt: "2025-05-04"
---

## 2. 벡터
### 2.1 벡터 집합
- 벡터의 모음을 벡터 집합이라고함. 아래와 같이 표시한다.
![벡터 집합 예시](/images/linear-algebra/2/1.png)

<br /> 

### 2.2 선형 가중 결합(linear weighted combination)
- 여러 변수마다 가중치를 다르게 주어 정보를 혼합하는 방법. 스칼라-벡터 곱셈을 한 다음 합하는것이다.

![선형 가중 결합](/images/linear-algebra/2/2.png)
- 모든 벡터(`vi`)의 차원은 같다고 가정한다.
- 가중 대신 계수(`coefficient`)를 쓰기도함.

<br /> 

### 2.3 선형 독립성
- 벡터 집합에서 적어도 하나의 벡터를 집합의 다른 벡터들의 선형 가중 결합으로 나타낼 수 있을때, ***벡터 집합을 선형 종속적(linearly dependent)*** 이라고 한다. ***반대가 선형 독립적(linearly independent)*** 이다.

- 선형 독립성을 알 수 있는 방법은 '벡터의 집합'으로 행렬을 만들고, '행렬의 계수'를 계산한 다음 행의 수와 열의 수 중 더 작은 값을 비교한다.(추후 다룸)

#### 2.3.1** 수학에서의 선형 독립성**
- **벡터 집합이 '선형 종속적'이라면, 선형 가중 결합으로 영 벡터를 만들 수 있다.** 식으로 나타내면 아래와 같다.(단 적어도 하나 이상의 가중치가 0이 아니다(`λ != 0`))
![선형 가중 결합](/images/linear-algebra/2/3.png)

- 0이 아닌 스칼라로 나누면 아래와 같이 바꿀수있다.
![선형 가중 결합](/images/linear-algebra/2/4.png)

#### 2.3.2 독립성과 영벡터
- 영백터가 포함된 모든 벡터 집합은 모두 선형 종속적이다. (영벡터의 스칼라만 0이 아니고, 나머지 스칼라가 0인경우)

### 2.4 부분공간(subspace)과 생성(span)
- 벡터 집합의 모든 무한한 선형 가중 결합으로 만들어지는 공간을 ***벡터 부분공간*** 이라고함. 이걸 만드는걸 ***생성*** 이라고 한다.
- 예시 1.
  - `V={[1, 3]}`으로 하나의 벡터를 가진 벡터집합의 부분공간은 아래와 같다.(2차원 평면에서의 1차원 선)
  - ![부분공강](/images/linear-algebra/2/5.png)

- 예시 2.
  - `V={[1,0,2], [-1,1,2]}`로 `R^3`에서 두개의 벡터를 가진 집합의 부분공간은 아래와 같다.(3차원 공간에서의 2차원 면)
  - ![부분공간](/images/linear-algebra/2/6.png)

- 예시 3.
  - `V={[1,1,1], [2,2,2]}`로 `R^3`에서 선형 종속적인 벡터집합의 부분공간은 아래와 같다. (3차원 공간에서의 1차원 선)
  - ![부분공간](/images/linear-algebra/2/7.png)

- 벡터 집합에서 생성되는 부분공간의 차원은 ***선형 독립 집합을 형성하는데 필요한 최소한의 벡터 수이다.*** 벡터 집합이 선형 독립적이면 부분공간의 차원은 벡터의 수와 동일해진다. 벡터 종속적이면 부분공간의 차원은 벡터의 수보다 작다.(정확한건 5장에서 행렬 계수를 알면 된다고함)
- '벡터 부분공간'은 ***덧셈과 스칼라 곱셈으로 닫혀 있는 부분집합으로, 공간의 원점(`λ=0`)을 포함한다.***

<br /> 

### 2.5 기저(basis)
- '기저'는 행렬의 정보를 설명하는데 사용하는 자(ruler)의 ***집합***.
- 대표적인 예가 데카르트 좌표계(Cartesian Coordinate System)이다. 서로 직교하면서 ***단위 길이***인 벡터로 이뤄져있다. 표준 기저 집합(standard basis set)이라고도 함
  - ![데카르트좌표계](/images/linear-algebra/2/8.png)
- 차원 축소, 특징 추출, 주성분 분석, 독립성분 분석, 데이터 압축.. 등의 모든 분석은 ***본질적으로 특정 문제에 대한 `최적의 기저벡터`를 찾는것이 목적***이다.
- 아래 그림은 데이터 집합이다. 그림에는 세개의 다른 기저를 보여준다.(`표준 기저 집합`, `주성분 분석(PCA)`, `독립성분 분석(ICA)`)
  - ![3가지기저](/images/linear-algebra/2/9.png)



#### 2.5.1 기저 정의
- 기저는 단순히 '생성'과 '독립성'을 결합한것이다. 
  - 벡터집합이 1) "특정 부분공간을 생성하고", 2) "독립적인 벡터 집합이라면" 해당 부분공간의 기저이다.
- 측정할 수 없는건 설명할 수 없기때문에, 기저가 어떤 부분공간의 기저가 되려면 해당 부분공간을 생성할 수 있어야한다. 아래 그림의 파란선 부분공간의 기저벡터는 점r을 측정할 수 없기때문에 기저벡터가 될 수 없다.
  - ![기저가 아니다](/images/linear-algebra/2/10.png)


- 기저 집합은 ***선형 독립적***이어야 한다. 부분공간의 모든 벡터는 기저를 이용한 고유한 좌표를 가져야 하기 때문이다. => ***하나의 기저 집합 내에서 하나의 점은 정확히 하나의 선형 가중 결합으로 된다.*** ( 하지만 하나의 점을 설명할 수 있는 기저 집합은 무한히 많다. )

<br /> 

### 2.6 정리
- `벡터 집합`은 벡터들의 모음이다. 집합에는 유한하거나 무한한 수의 벡터가 존재한다.
- ***`선형 가중 결합`은 집합의 벡터들에 스칼라를 곱하고 합하는 것이다.***
- 집합에서 하나의 벡터가 집합의 다른 벡터들의 선형 가중 결합으로 기술될 수 있으면 해당 벡터 집합은 `선형 종속적`이다. 이러한 선형 가중 결합이 없다면 집합은 `선형 독립적`이다.
- `부분공간`은 벡터 집합의 가능한 모든 선형 가중 결합으로 만들어진 무한 집합이다.
- `기저`는 공간을 측정하기 위한 일종의 자이다. 벡터 집합이 (1) 어떤 부분공간을 생성하고 (2) 선형 독립적이라면 해당 부분공간에 대한 기저가 된다. 데이터 과학의 주요 목표는 데이터 집합을 설명하거나 문제를 해결하기 위한 최상의 기저 집합을 찾는것이다.

<br /> 

## 3. 벡터 응용: 데이터 분석에서의 벡터
### 3.1 상관관계와 코사인 유사도
- 상관계수(`correlation coefficient`) 
  - 두 변수 사이의 "선형 관계"를 정량화한 숫자이다.
  - -1 ~ +1 사이의 값이다.
    - -1: 완벽한 음의관계
    - +1: 완벽한 양의관계
    - 0: 관계없음

- 상관계수가 기대하는 범위 내에 존재하려면 정규화가 필요하다. 아래는 피어슨 상관계수(`Pearson correlation coefficient)` 를 구하는 방법이다.
  - `평균 중심화` : 각 데이터값에서 평균값을 뺀다.
    - ![중심화](/images/linear-algebra/3/3.png)
  - `벡터 노름 곱으로 내적을 나누기` : 분할(divisive)정규화는 측정 단위를 제거하고 상관계수 최대 크기를 `|1|` **로 조정한다.**
  
    - ![피어슨 상관계수](/images/linear-algebra/3/1.png)
  - 이걸 내적으로 풀면 아래와 같이 바꿀수있다.
    - ![과정](/images/linear-algebra/3/4.png)
    - ![피어슨 상관계수(내적)](/images/linear-algebra/3/2.png)


- 다른 방법으로 ***코사인 유사도***(`cosine similarity`)가 있다. 코사인 유사도는 단순히 내적의 기하학적 공식에서 코사인 값을 좌변에 놓은것이다.(a가 x,y의 내적)
  - ![코사인 유사도](/images/linear-algebra/3/5.png)


- 상관관계와 코사인 유사도의 차이
  - 피어슨 상관관계에서 [0,1,2,3]과 [100,101,102,103] 은 완벽한 상관관계를 가진다.(`ρ = 1`). 한 변수가 더 큰 숫자라는건 중요하지 않고 똑같이 변한다는게 중요하다.
  - 코사인유사도는 0.808이다.동일한 숫자 척도가 아니라서 완벽한 상관관계가 아니다.

- 피어슨 상관관계와 코사인 유사도는 둘 다 '내적' 기반이고, 내적은 선형 연산이기 때문에 두 변수 사이의 선형 관계를 반영하는 것이다.

<br /> 

### 3.2 시계열 필터링과 특징 탐지
- 생략

<br /> 

### 3.3 k-평균 클러스터링
- k-평균 클러스터링(`k-means clustering`)은 그룹 중심까지의 거리를 최소화하도록 다변량 데이터를 상대적으로 적은 수(k)의 그룹 또는 범주로 분류하는 비지도 기법이다.
- 방법
  1. 데이터 공간에서 임의의 k개 중심점을 초기화한다. 중심을 `클래스` 또는 `범주`라고 하고 한다.
  2. 각 데이터 관측치와 각 중심 사이의 유클리드 거리를 계산한다.
  3. 각 데이터 관측치를 가장 가까운 중심의 그룹에 할당한다.
  4. 각 중심을 해당 중심에 할당된 모든 데이터의 평균으로 갱신한다.
  5. 수렴 기준을 만족할 때까지 2~4를 반복한다.


<br /> 

## 4. 행렬, 파트1: 행렬과 행렬의 기본 연산
### 4.1 NumPy에서 행렬 생성과 시각화
### 4.1.1 행렬 시각화, 인덱싱, 슬라이싱
- 작은 행렬은 값을 다 표현할 수 있지만 data science에서는 크기가 너무 커서 이미지로 시각화함.(e.g. `mat-plot-lib`)
  - ![행렬 시각화](/images/linear-algebra/4/1.png)

- NumPy를 쓰면 다양한 행렬을 쉽게 만들수있다. 파이썬은 슬라이싱도 쉽게해줌
```python
import numpy as np

# 행렬 생성(6*10)
A = np.arange(60).reshape(6, 10)
# 행렬 슬라이싱(1~3행, 1~5열)
sub = A[1:4:1, 0:5:1]
```

### 4.1.2 특수 행렬
- ![다양한 특수행렬](/images/linear-algebra/4/2.png)

- 난수행렬(random numbers matrix): 가우스 분포같은것에서 무작위로 추출
```python
# 난수 행렬
Mrows = 4 
Ncols = 6
A = np.random.rand(Mrows, Ncols)
```

- 정방행렬 / 비정방 행렬
  - 정사각형: 정방, 그 외 : 비정방

- 대각 행렬
  - 행렬의 대각(`diagonal`) 원소 외에는 모두 0인 행렬

- 삼각 행렬
  - 주 대각선 위or아래가 모두 0인원소
  - 위가 0이면 상삼각, 아래가 0이면 하삼각


- 단위 행렬(`identity matrix`)
  - 어떤 행렬/벡터에 단위 행렬을 곱했을 때 그 행렬이 나오는 것. 1과 같다.
  - 모든 원소가 1인 정방 대각 행렬

- 영 행렬
  - 모든 원소가 0인 행렬

<br />

### 4.2 행렬 수학: 덧셈, 스칼라 곱셈, 아다마르곱
### 4.2.1 덧셈과 뺄셈
- 덧셈/뺄샘은은 크기가 같은 두 행렬 사이에서만 성립. 각 원소를 더하고 빼면됨

### 4.2.2 행렬 '이동'(shifting)
- "정방 행렬"에 "대각선 원소"에 스칼라를 더하는걸 행렬 이동이라고 함
  - ![행렬 이동](/images/linear-algebra/4/3.png)
  - A는 정방행렬, λ는 스칼라, I는 단위행렬

- 이게 직관적으로 이해는 안되는데, 정방행렬의 `고유값`을 이동시키기 위한 수단이라고함.(나중에 나온다고)
  - ![고유값](/images/linear-algebra/4/4.png)
- 데이터 과학에서는 행렬의 수치적 안정성을 높이고 이동의 효과를 보면서 가능한 많은 정보를 보존하기 위한 거라고 함.

### 4.2.3 스칼라 곱셈과 아다마르곱
- 스칼라 곱셈: 각 원소에 동일한 스칼라를 곱한다.
- 아다마르곱: 두개의 똑같은 크기의 행렬의 각 원소를 곱한다.
  - ![아다마르곱](/images/linear-algebra/4/5.png)
```python
A = np.random.rand(3,4)
B = np.random.rand(3,4)
print(A*B)
print(np.multiply(A,B))
```

<br />

### 4.3 표준행렬곱