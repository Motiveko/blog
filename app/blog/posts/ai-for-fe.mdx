---
title: "AI 개념정리"
description: "프론트앤드 개발자 수준에서의 AI 개념정리"
keywords:["AI", "머신러닝", "딥러닝", "머신러닝 개념", "ai 개념", "ai 트랜드", "알고리즘", "RNN", "LSTM", "GRU", "Transformer", "BERT", "GPT"]
lang: "ko"
date: "2025-04-02"
publishedAt: "2025-04-02"
---


> 

# 


## 딥러닝
### 퍼셉트론
### MLP(Multi-Layer Perceptron, 다중 퍼셉트론)
- 구조
  - 입력층
  - 은닉층
  - 출력층
- 



### CNN (Convolutional Neural Network, 합성곱 신경망)
{/* https://gemini.google.com/app/5cca21e18435f268?hl=ko */}

- 구조
  - 입력층
  - 합성곱 계층(Convolutional Layer)
  - 풀링층
  - 출력층


<br/>

### RNN (Recurrent Neural Network, 순환 신경망)
{/* https://gemini.google.com/app/f510ecddc8a5a2b1?hl=ko */}
- 순서가 중요한 순차 데이터(sequential) 처리시 MLP는 순서를 고려하지 않는다는 한계가 있음.
  - 문장 (`나는 밥을 먹었다 <=> 밥은 나를 먹었다.`)
  - 주식데이터, 비디오프레임

- `RNN`은 `MLP`에서 `Hidden State`(은닉 상태)라는게 추가로 존재하는 형태임. 이전 단계의 출력이 다음 단계의 입력으로 사용되는 순환 구조
- 동작 시퀀스( 입력: 'hello' )
```markdown
# 1. 시점 t=1 (첫 번째 글자 'h'):
- 입력: 첫 번째 입력 데이터 x1 ('h'를 나타내는 벡터)
- 이전 은닉 상태: 첫 시점이므로, 이전 은닉 상태 h0는 보통 0 또는 무작위 값으로 초기화
- 계산: RNN 셀은 입력 x1과 이전 은닉 상태 h0를 함께 사용하여 현재 시점의 은닉 상태 h1을 계산 (여기서 가중치 W_xh와 W_hh가 사용)
- 출력 (선택적): 이 h1을 이용하여 현재 시점의 출력 y1을 계산할 수도 있음. (예: 'h' 다음에 올 글자를 예측한다면, 'e'일 확률이 가장 높게 나오는 것이 목표일 수 있따.)
핵심: 계산된 h1은 'h'라는 글자에 대한 정보를 담고 있습니다.
```

```markdown
# 2. 시점 t=2 (첫 번째 글자 'e'):
- 입력: 두 번째 입력 데이터 x2 ('e'를 나타내는 벡터)가 들어옴
- 이전 은닉 상태: 바로 이전 시점(t=1)에서 계산된 은닉 상태 h1을 가져옴. RNN에서 기억하는 이전 내용이 이거임(시퀀셜 데이터)
- 계산: RNN 셀은 입력 x2 ('e')와 **이전 은닉 상태 h1 ('h'의 정보)**을 함께 사용, 현재 시점의 은닉 상태 h2를 계산
- 출력 (선택적): h2를 이용하여 현재 시점의 출력 y2를 계산. (예: 'he' 다음에 올 글자를 예측. 'l'일 확률이 높다)
- 핵심: h2는 'h'와 'e'를 순서대로 처리한 정보, 즉 'he'까지의 문맥 정보를 담게됨
```

... 

```markdown
# 5. 시점 t=2 (첫 번째 글자 'e'):
- 입력: x5 ('o')
- 이전 은닉 상태: h4 ('hell'의 정보)
- 계산: x5와 h4를 사용하여 h5 계산.
- 출력 (선*택적): y5 계산. (예: 'hello' 다음에 올 글자 예측)
- 핵심: ***h5는 'hello' 전체의 문맥 정보를 담게 됨***
```

- RNN 장점
  - 가변 길이의 순차데이터 처리가능
  - 이전 정보를 현재 계산에 반영해서 ***"문맥"*** 을 파악할 수 있음

- RNN 단점
  - 순서가 길어지면 오래전 정보는 잘 기억 못함(기울기 소실 문제, `Vanishing Gradient Problem`)
    - 다른말로 `Long-Term Dependency Problem`이라고 함.
    - 기울기가 너무 작아지거나(vanishing) 너무 커지거나(exploding) 해서 모델이 학습이 안되는 문제가 발생함.
  - 이걸 해결하는게 `LGTM`, `GRU` 같은 더 발전된 형태의 RNN

<br />

### LSTM (Long Short-Term Memory)
{/* https://gemini.google.com/app/1a9443987b255e62?hl=ko */}
- `RNN`의 기울기 소실 문제를 해결하기 위해 고안된 모델. 쓸모있는 정보는 잊어버리고, 중요한 정보는 오래 기억하자.
- 셀 상태(Cell State)라는 정보 흐름과 이걸 제어하는 게이트(Gate)를 추가하는 방식으로 구현함
- 구성
  - **셀 상태(Cell State, `C_t`)**
  - **게이트(Gates)**
    - 시그모이드(Sigmoid)함수를 이용해서 0~1 사이 값 출력해서 정보의 흐름을 열고닫음(0: 닫음, 1: 열림)
    - **망각 게이트(Forget Gate)**
      - 과거 정보(`C_{t-1}`)에서 얼마나 잊어버릴지 결정, 0에 가까울수록 잊어버린다.(`f_t`)
      - 이전 시점의 은닉 상태(`h_{t-1}`)와 현재 입력(`x_t`)을 받아서 결정
    - 입력 게이트(Input Gate)
      - 이전 시점의 은닉 상태(`h_{t-1}`)와 현재 입력(`x_t`)을 받아서 어떤 새로운 정보를 셀 상태에 추가할지 결정
      - 시그모이드 레이어: 어떤 값을 업데이트할지 결정(`i_t`)
      - tanh 레이어: 셀 상태에 추가될 새로운 후보 값 벡터(`~C_t`) 생성.
    - 출력 게이트(Output Gate)
      - 업데이트된 셀 상태(`C_t`) 중에서 어떤 정보를 현재 시점의 은닉 상태(`h_t`)로 내보낼지 결정(`o_t`)
      - 이전 은닉 상태(`h_{t-1}`)와 현재 입력(`x_t`)을 받아서 결정

- 동작 시퀀스
  - "이 영화는 초반에는 약간 지루했지만, 후반부 액션과 스토리는 정말 최고였다."라는 문장의 감성분석 예시(긍정/부정)
```markdown
# 1. "이 영화는"
- 셀이 첫 단어 입력받음
- 초기 셀 상태와 은닉 상태가 업데이트됨
```

```markdown
# 2. "초반에는 약간 지루했지만"
- "지루했지만"에서 부정적인 감성 감지
- 입력 게이트(`i_t`)는 부정적 정보를 셀 상태에 추가하도록 결정할 가능성 높음(sigmoid 값이 1에 가깝다)
- 망각 게이트(`f_t`)는 아직 특별히 잊을 정보가 없어서 이전 정보를 유지함.(sigmoid 값이 1에 가깝게)
- 셀 상태(`C_t`)는 부정적 감정 정보를 가지게됨
- 출력 게이트(`o_t`)는 이 부정적 정보를 반영하여 현재 은닉 상태(`h_t`)를 출력
```

```markdown
# 3. "후반부 액션과 스토리는"
- 중립적이거나, 혹은 약간 긍적일 수 있음
- 입력 게이트(`i_t`)는 "후반부", "액션", "스토리" 등 정보를 셀 상태에 추가할지 결정함
- 망각 게이트(`f_t`)는 "지루했지만"의 부정 정보가 유효하다고 판단할 수도 있고, 반전을 예상해서 잊을 수도 있음(sigmoid 값이 1에 가깝게)
- 셀 상태(`C_t`)는 업데이트됨, 여전히 부정적 정보가 남아있을 수 있음
```

```markdown
# 4. "정말 최고였다."
- 매우 긍정적인 감성 감지
- 망각 게이트(`f_t`)는 긍정적 감성을 보고 이전에 저장했던 부정적 정보는 잊어버리도록 결정할 수 있다("지루했지만"에 해당하는 셀 상태 정보에 0에 가까운 값을 곱함)
- 입력 게이트(`i_t`)는 긍정적인 정보를 셀 상태에 추가하도록 결정한다.(sigmoid 값이 1, ~C_t는 긍정 벡터가 된다)
- 셀 상태(`C_t`)는 부정적인 정보가 사라지고 긍정적인 정보로 업데이트욈
- 출력 게이트(`o_t`)는 긍정적인 감정을를 나타내는 은닉 상태(`h_t`)를 출력
```

- 셀 상태는 레이어 단위로 관리됨, 레이어에 128개의 유닛이 있으면 128차원 벡터.
- 각 벡터의 상태는 입력 시퀀스 갯수 만큼의 벡터 시퀀스를 가지게됨(일종의 list)

<br />


### GRU (Gated Recurrent Unit)
{/* https://gemini.google.com/app/fa52d9bb59c1760f?hl=ko */}
- `LSTM`의 복잡한 게이트 구조를 단순화한 모델, 두개의 게이트만 사용한다
- 구성
  - 업데이트 게이트(Update Gate, `r_t`)
    - 과거의 정보를 얼마나 잊을지 결정. 0에 가까우면 잊게됨
  - 리셋 게이트(Reset Gate, `z_t`)
    - 과거의 정보를 얼마나 유지할지 결정
    - 망각 게이트과 입력게이트의 합친것과 비슷함.
    - 0에 가까우면 과거 정보를 무시하고 새로 계산된 정보로 대체함
  

- 시점 `t`에서의 입력(`x_t`)과 이전 시점 `t-1`의 은닉 상태(`h_{t-1}`)를 가지고 현재 시점 `t`의 은닉 상태(`h_t`)를 계산하는 과정
1. 리셋 게이트(`r_t`) 계산. 
- 이전 은닉 상태(`h_{t-1}`)와 현재 입력(`x_t`)을 받아서 결정. `σ`는 시그모이드 함수
```markdown
r_t = σ(W_r * [h_{t-1}, x_t])
```

2. 업데이트 게이트(`z_t`) 계산. 
- 이전 은닉 상태(`h_{t-1}`)와 현재 입력(`x_t`)을 받아서 결정. `σ`는 시그모이드 함수
```markdown
z_t = σ(W_z * [h_{t-1}, x_t])
```


3. 후보 은닉 상태(Candidate Hidden State, `h̃_t`) 계산. 
- 이전 은닉 상태(`h_{t-1}`)와 현재 입력(`x_t`)을 받아서 결정. `tanh`는 하이퍼볼릭 탄젠트 함수.
```markdown
h̃_t = tanh(W_h * [(r_t * h_{t-1}), x_t])
```

4. 최종 은닉 상태(`h_t`) 계산. 
- 업데이트 게이트를 사용해서 과거 정보(`h_{t-1}`)와 후보 은닉 상태(`h̃_t`)를 혼합

```markdown
h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t
```
- 업데이트 게이트(`z_t`)가 1에 가까우면 과거 정보(`h_{t-1}`)를 유지하고, 0에 가까우면 후보 은닉 상태(`h̃_t`)를 선택함.

<br />

- 동작 시퀀스
  - "이 영화 정말 재미있었지만, 결말은 좀 아쉬웠어"의 감성분석
```markdown
# 1. "이 영화 정말 재미있었지만"
- 은닉 상태는 긍정적인 정보를 축적함.
- 업데이트 게이트(z)는 새로운 긍정을 받아들이고, 이전 긍정 상태를 유지할 가능성이 높음
```

```markdown
# 2. "결말은"
- 문맥 전환 가능성 생김
- 리셋 게이트(r)가 활성화 되어 긍정 상태를 약간 리셋하고 결말이라는 새로운 주제에 집중함
```

```markdown
# 3. "좀 아쉬웠어"
- 부정적인 감정 감지
- 리셋 게이트(r)가 약간 리셋된 이전 상태와 부정적인 입력값을 조합해서 부정적인 후보상태를 만듬
- 업데이트 게이트(z)가 이전의 긍정적 상태(h_{t-1})를 얼마나 유지하고, 새롭게 계산된 부정적 후보상태(h̃_t)를 얼마나 반영할지 결정함. `z_t`값이 커져서 부정 상태에 가깝게 업데이트 될 거같음
- 최종적으로 `h_t`는 긍정적인 면도 있었지만 결론저긍로 아쉽다는 복합적인 감성정보를 담게됨.
```


- `LSTM`과 비교
  - 게이트 수가 다름
  - **`LSTM`은 별도의 셀 상태를 사용해서 장기 기억을 관리하지만, GRU는 은닉 상태가 이 역할을 겸함**
  - `GRU` 구조가 더 간단하고 파라미터 수가 적어 빠를수있음










### Transformer
### BERT
### GPT

